{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import librosa\n",
    "import os\n",
    "from skimage import io\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset\n",
    "import math\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from pandas import Series\n",
    "import IPython.display as ipd\n",
    "## Reconstructing File Paths\n",
    "\n",
    "file_path = 'Datasets/LibriSpeech/train-clean-100.index.csv'\n",
    "\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "df.columns\n",
    "\n",
    "df.head()\n",
    "\n",
    "df.shape\n",
    "\n",
    "\n",
    "df.filepath[1]\n",
    "\n",
    "df.filepath[1][0:38]\n",
    "\n",
    "mine = '/home/amonajemi/amonajemi/Datasets'\n",
    "\n",
    "df['filepath'] = df['filepath'].str[37:]\n",
    "\n",
    "df['filepath'] = mine + df['filepath']\n",
    "\n",
    "df['filepath'].head()\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "df.head()\n",
    "\n",
    "\n",
    "class LibriSpeechData(Dataset):\n",
    "    def __init__(self,df,transform = None, sampling_rate = 8000, offset=1.0, duration=5.0):\n",
    "        self.duration = duration\n",
    "        self.df = df[df.seconds > self.duration].reset_index()\n",
    "        self.audio = self.df['filepath']\n",
    "        self.identities=self.df['id']\n",
    "        self.transform = transform\n",
    "        self.sampling_rate = sampling_rate\n",
    "        if self.transform is None:\n",
    "            self.transform = torch.from_numpy # Call it as an object! Now transform is defined so \n",
    "                                                # we cn iterate through the batches. \n",
    "\n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        audio = self.audio[index]\n",
    "        self.seconds = self.df['seconds'][index]\n",
    "        self.offset = random.uniform(0,self.seconds - self.duration)\n",
    "        y,  sr  = librosa.load(audio, sr=self.sampling_rate, offset=self.offset, duration=self.duration)\n",
    "        # The y is the one-dimensional np.array\n",
    "        #that is the amplitude of the waveform at sample t\n",
    "        \n",
    "        #the sr is the number of samples per second of a\n",
    "        #time series.\n",
    "    \n",
    "        time_series = self.transform(y)\n",
    "        \n",
    "        return time_series, torch.tensor(self.identities[index])        \n",
    "\n",
    "df.index # Our filtering of the dataset kept the original index. This is why you see 28437 here but the\n",
    "#overall length of indexes is indeed 26504 \n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa.display\n",
    "\n",
    "def waveplot(y, r):\n",
    "    plt.figure(figsize=(14, 5))\n",
    "    librosa.display.waveplot(y, sr=r)\n",
    "\n",
    "\n",
    "def spectrogram_plot(y):\n",
    "    X = librosa.stft(y)\n",
    "    Xdb = librosa.amplitude_to_db(abs(X))\n",
    "    plt.figure(figsize=(14, 5))\n",
    "    librosa.display.specshow(Xdb, sr=r, x_axis='time', y_axis='log')\n",
    "    \n",
    "def energy_plot(a, hop_length = 256, frame_length = 2048):\n",
    "    energy = np.array([\n",
    "        sum(abs(a.numpy()[i:i+frame_length]**2))\n",
    "        for i in range(0, len(a.numpy()), hop_length)])\n",
    "    return plt.plot(energy / energy.max()), a.min(),a.max()\n",
    "\n",
    "## Energy Detection \n",
    "\n",
    "def energy_values(a, hop_length = 256, frame_length = 2048):\n",
    "    energy = np.array([\n",
    "        sum(abs(a.numpy()[i:i+frame_length]**2))\n",
    "        for i in range(0, len(a.numpy()), hop_length)])\n",
    "    return energy\n",
    "def energy_values(a, hop_length = 256, frame_length = 2048):\n",
    "    energy = np.array([\n",
    "        sum(abs(a[i:i+frame_length]**2))\n",
    "        for i in range(0, len(a), hop_length)])\n",
    "    return energy\n",
    "\n",
    "get_energy = energy_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "Voices = pd.read_csv(\"/home/amonajemi/amonajemi/Datasets/VOiCES/VOiCES-room-1.index.csv\")\n",
    "\n",
    "Voices.columns\n",
    "\n",
    "Voices['filepath'][40000]\n",
    "\n",
    "Voices['filepath'][1][22:]\n",
    "\n",
    "file_e = '/home/amonajemi/amonajemi/Datasets/VOiCES/Volumes/'\n",
    "\n",
    "Voices['filepath'] = Voices['filepath'].str[22:]\n",
    "\n",
    "Voices['filepath'] = file_e + Voices['filepath']\n",
    "    \n",
    "\n",
    "Voices = Voices[~Voices.filepath.str.contains(\"tele\")]\n",
    "\n",
    "len(Voices)\n",
    "\n",
    "Voices.index = range(0,len(Voices))\n",
    "\n",
    "Voices = Voices[(Voices.filepath.str.contains(\"sp_3549\") & Voices.filepath.str.contains(\"none\"))|(Voices.filepath.str.contains(\"sp_6241\") & Voices.filepath.str.contains(\"none\")) | (Voices.filepath.str.contains(\"sp_1212\") & Voices.filepath.str.contains(\"musi\")) | (Voices.filepath.str.contains(\"sp_6241\") & Voices.filepath.str.contains(\"musi\"))]\n",
    "\n",
    "Voices.index = range(0,len(Voices))\n",
    "\n",
    "filename = random.choice(Voices['filepath'])\n",
    "y , sr = librosa.load(filename)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class VoicesDataSet(Dataset):\n",
    "    def __init__(self,df,transform = None, sampling_rate = 8000, offset=1.0, duration=3.0):\n",
    "        self.duration = duration\n",
    "        self.df = df[df.seconds > self.duration].reset_index()\n",
    "        self.audio = self.df['filepath']\n",
    "        self.identities=self.df['id']\n",
    "        self.transform = transform\n",
    "        self.sampling_rate = sampling_rate\n",
    "        if self.transform is None:\n",
    "            self.transform = torch.from_numpy # Call it as an object! Now transform is defined so \n",
    "                                                # we cn iterate through the batches. \n",
    "\n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        audio = self.audio[index]\n",
    "        self.seconds = self.df['seconds'][index]\n",
    "        self.offset = random.uniform(0,self.seconds - self.duration)\n",
    "        y,  sr  = librosa.load(audio, sr=self.sampling_rate, offset=self.offset, duration=self.duration)\n",
    "        # The y is the one-dimensional np.array\n",
    "        #that is the amplitude of the waveform at sample t\n",
    "        \n",
    "        #the sr is the number of samples per second of a\n",
    "        #time series.\n",
    "    \n",
    "        time_series = self.transform(y)\n",
    "        \n",
    "        return time_series, torch.tensor(self.identities[index])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fade In / Fade Out\n",
    "import statistics\n",
    "def fading_in(length):\n",
    "    fading_in =np.linspace(0, 1, length)\n",
    "    #fading_in[int(length*.50):] = 1\n",
    "    return fading_in\n",
    "\n",
    "def fading_out(length):\n",
    "    fading_out = np.linspace(0,1, length)[::-1]\n",
    "    #fading_out[:int(length * .50)] = 1\n",
    "    return fading_out\n",
    "\n",
    "\n",
    "\n",
    "# VAD\n",
    "\n",
    "def zero_runs(a):\n",
    "    # Create an array that is 1 where a is 0, and pad each end with an extra 0.\n",
    "    iszero = np.concatenate(([0], np.equal(a, 0).view(np.int8), [0]))\n",
    "    absdiff = np.abs(np.diff(iszero))\n",
    "    # Runs start and end where absdiff is 1.\n",
    "    ranges = np.where(absdiff == 1)[0].reshape(-1, 2)\n",
    "    greatest = 0\n",
    "    start = 0       # For Voices change this to the max interval again like below\n",
    "    end = 0\n",
    "    for interval in ranges:\n",
    "        diff = interval[1] - interval[0]\n",
    "        if diff > greatest:\n",
    "            greatest = diff\n",
    "            start = interval[0]\n",
    "            end = interval[1]\n",
    "    return list([start, end - 1])\n",
    "\n",
    "def zero_runs_full(a):\n",
    "    # Create an array that is 1 where a is 0, and pad each end with an extra 0.\n",
    "    iszero = np.concatenate(([0], np.equal(a, 0).view(np.int8), [0]))\n",
    "    absdiff = np.abs(np.diff(iszero))\n",
    "    # Runs start and end where absdiff is 1.\n",
    "    ranges = np.where(absdiff == 1)[0].reshape(-1, 2)\n",
    "    return ranges\n",
    "\n",
    "# Step 1 : Take an instance of the Voices Class\n",
    "def Voices_data_Speech(dataset_class, _data):\n",
    "    data = dataset_class(_data)\n",
    "    #global a\n",
    "    a,b = data[random.choice(_data.index)]\n",
    "    vad = VAD(a.numpy(), sr = 8000, theshold = .30)\n",
    "    zero_runs_intervals = zero_runs_full(vad)\n",
    "    sorted_silences = sorted(zero_runs_intervals,  key = lambda arr :  arr[1] - arr[0], reverse = True)[:2]\n",
    "    # Can make a threshold here of what the requirement of length is for a silent window (eg : 4000)\n",
    "    #If there is only one silence interval, then no need for the energy loop.\n",
    "    max_energy = 1000\n",
    "    start = 0\n",
    "    end = 0\n",
    "    for interval in sorted_silences:\n",
    "        energy_amount = statistics.mean(get_energy(a.numpy()[interval[0] : interval[1]]))\n",
    "        if energy_amount <= max_energy:\n",
    "            max_energy = energy_amount\n",
    "            start = interval[0]\n",
    "            end = interval[1] - 1\n",
    "    #plt.plot(vad)\n",
    "    #plt.plot(a.numpy())\n",
    "    return list([start, end])\n",
    "\n",
    "#Voices_silent_interval = Voices_data_Speech(VoicesDataSet, Voices)\n",
    "\n",
    "#Voices_silent_interval\n",
    "\n",
    "#full_Voices = a.numpy()\n",
    "\n",
    "#ipd.Audio(full_Voices[Voices_silent_interval[0] : Voices_silent_interval[1]], rate = 16000)\n",
    "\n",
    "def one_runs(a):\n",
    "    # Create an array that is 1 where a is 0, and pad each end with an extra 0.\n",
    "    iszero = np.concatenate(([0], np.equal(a, 0).view(np.int8), [0]))\n",
    "    absdiff = np.abs(np.diff(iszero))\n",
    "    # Runs start and end where absdiff is 1.\n",
    "    ranges = np.where(absdiff == 1)[0].reshape(-1, 2)\n",
    "    interval = []\n",
    "    for i in range(len(ranges) - 1):\n",
    "        start_one = ranges[i][1]\n",
    "        upper = i+1\n",
    "        end_one = ranges[upper][0]-1\n",
    "        interval.append(start_one)\n",
    "        interval.append(end_one)\n",
    "    ranges_ones = np.array(interval).reshape(-1,2)\n",
    "    \n",
    "    return ranges_ones # sorted(ranges_ones, key = lambda arr :  arr[1] - arr[0], reverse = True)\n",
    "    \n",
    "\n",
    "# Step 1 : Take an instance of the LibriSpeech Class\n",
    "def Libri_data_Speech(dataset_class, _data, padding, length_needed):\n",
    "    Flag = True\n",
    "    while Flag:\n",
    "        data = dataset_class(_data)\n",
    "        global y\n",
    "        y, c  = data[random.choice(_data.index)]\n",
    "        vad = VAD(y.numpy(), sr = 8000, theshold = .30)\n",
    "        interval_ones = one_runs(vad)\n",
    "        interval_zeros = zero_runs_full(vad)\n",
    "        sorted_intervals = sorted(interval_ones, key = lambda arr : arr[1] - arr[0], reverse = True)\n",
    "        new_intervals = [i for i in sorted_intervals if 40000 not in i and 0 not in i and i[1] - i[0]>=length_needed] \n",
    "        selected_interval=0\n",
    "        for i in new_intervals:\n",
    "            start_left = interval_zeros[np.where(interval_zeros==i[0])[0]][0][0]\n",
    "            end_left = interval_zeros[np.where(interval_zeros==i[0])[0]][0][1] - 1\n",
    "            start_right = interval_zeros[np.where(interval_zeros==i[1] + 1)[0]][0][0]\n",
    "            end_right = interval_zeros[np.where(interval_zeros==i[1] + 1)[0]][0][1] - 1\n",
    "            if end_left - start_left >=padding and end_right - start_right >=padding:\n",
    "                #consider adding a mean energy requirement for each interval of padding.\n",
    "                selected_interval = i\n",
    "                plt.plot(vad)  # plotting vad\n",
    "                plt.plot(y.numpy()) # plotting time series\n",
    "                Flag = False\n",
    "                break\n",
    "            #return as variable\n",
    "    return list([selected_interval[0] - (end_left - start_left) , selected_interval[1] + (end_right - start_right)])\n",
    "\n",
    "#Libri_Speech = Libri_data_Speech(LibriSpeechData, df, padding = 1400, length_needed = 8000)\n",
    "#print(Libri_Speech[0])\n",
    "#print(Libri_Speech[1])\n",
    "#Libri_Speech = y.numpy()[Libri_Speech[0] : Libri_Speech[1]] # We have the indices of the loud librispeech.\n",
    "# now we \n",
    "\n",
    "\n",
    "def make_new_audio(Voices_data, Lib_data, silent_begin, silent_end, silent_interval):\n",
    "    if silent_begin is not 0:\n",
    "        sample_audio = Voices_data[:int(silent_begin)]\n",
    "       # Voices_Segment1 = np.concatenate((sample_audio[:int(len(sample_audio) * 0.50)] , \n",
    "        #                         sample_audio[int(len(sample_audio) * 0.50):] \n",
    "          #                       * np.linspace(0,1,math.ceil(len(sample_audio) * 0.50))[::-1]))\n",
    "        \n",
    "        Voices_Segment1 = Voices_data[:int(silent_begin)] #* fading_out(len(Voices_data[:int(silent_begin)]))\n",
    "    \n",
    "    half1_silent = Voices_data[int(silent_begin) : int(statistics.mean(silent_interval))]\n",
    "    fade_out_half1 = half1_silent * fading_out(len(half1_silent))\n",
    "    \n",
    "    Libri_audio_half1 = Lib_data[:int(math.floor(len(Lib_data) / 2))]\n",
    "    Libri_audio_half1 = Libri_audio_half1 #* fading_in(len(Libri_audio_half1)) # may not need to fade here \n",
    "    \n",
    "    Libri_audio_half2 = Lib_data[int(math.floor(len(Lib_data) / 2)):]\n",
    "    Libri_audio_half2 = Libri_audio_half2 #* fading_out(len(Libri_audio_half2)) # may not need to fade here\n",
    "    \n",
    "    Libri_full = np.concatenate((Libri_audio_half1 , Libri_audio_half2))\n",
    "    \n",
    "    half2_silent = Voices_data[int(statistics.mean(silent_interval)):int(silent_end)]\n",
    "    fade_in_half2 = half2_silent* fading_in(len(half2_silent))\n",
    "    \n",
    "    \n",
    "    sample_audio1 = Voices_data[int(silent_end):]\n",
    "    #Voices_Segment2 = np.concatenate((sample_audio1[:int(len(sample_audio1) * 0.50)] \n",
    "     #                               * np.linspace(0,1,int(len(sample_audio1) * .50)), \n",
    "      #                           sample_audio1[int(len(sample_audio1) * 0.50):]))\n",
    "    \n",
    "    Voices_Segment2 = Voices_data[int(silent_end):] #* fading_in(len(Voices_data[int(silent_end):]))\n",
    "    if Voices_Segment1 is not None: # Put this not .all()\n",
    "        new_audio = np.concatenate((Voices_Segment1, fade_out_half1,Libri_full, fade_in_half2, Voices_Segment2))\n",
    "    else:\n",
    "        new_audio = np.concatenate((fade_out_half1, Libri_full, fade_in_half2, Voices_Segment2))\n",
    "        \n",
    "    return new_audio\n",
    "\n",
    "def audio_frames(Voices_data, Lib_data, silent_begin, silent_end, silent_interval):\n",
    "    # create zeros and ones for frames \n",
    "    _Voices_seg1 = np.zeros(len(Voices_data[:int(silent_begin)]))\n",
    "    _silent1 = np.zeros(len(Voices_data[int(silent_begin) : int(statistics.mean(silent_interval))]))\n",
    "    _Libri = np.ones(len(Lib_data))\n",
    "    _silent2 = np.zeros(len(Voices_data[int(statistics.mean(silent_interval)) : int(silent_end)]))\n",
    "    _Voices_seg2 = np.zeros(len(Voices_data[int(silent_end):]))\n",
    "    frames = np.concatenate((_Voices_seg1, _silent1, _Libri, _silent2, _Voices_seg2))\n",
    "    #plt.plot(frames)\n",
    "    return frames #one_runs(frames) return this if you want the start and end points only\n",
    "    \n",
    "\n",
    "def normalizer(data):\n",
    "    series = Series(data)\n",
    "    values = series.values\n",
    "    values = values.reshape((len(values), 1))\n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "    scaler = scaler.fit(values)\n",
    "    normalized = scaler.transform(values)\n",
    "    normalized = normalized.reshape(-1)\n",
    "    return normalized\n",
    "\n",
    "def get_dist_libri(n_iter):\n",
    "    ratio = []\n",
    "    for i in range(n_iter):\n",
    "        sample = v[random.choice(mp.df.index)]\n",
    "        ratio.append(sum(sample[1].numpy()) / len(sample[1]))\n",
    "    return ratio\n",
    "\n",
    "\n",
    "def truncate_audio_variable_length(new_audio, synthetic_frames, upper , lower = None) :\n",
    "    if lower is None:\n",
    "        truncate_size = upper\n",
    "    else:\n",
    "        truncate_size = random.randint(upper, lower)\n",
    "    \n",
    "    right_prop_libri = sum(synthetic_frames[0 : truncate_size]) / len(synthetic_frames[0 : truncate_size])\n",
    "\n",
    "    left_prop_libri = sum(synthetic_frames[-1 * truncate_size : ]) / len(synthetic_frames[-1*truncate_size:])\n",
    "    \n",
    "    if abs(right_prop_libri - 0.50) <= abs(left_prop_libri - 0.50):\n",
    "        truncated_audio = new_audio[0 : truncate_size]\n",
    "        synthetic_frames_truncated_audio = synthetic_frames[0:truncate_size]\n",
    "        \n",
    "    else:\n",
    "        truncated_audio = new_audio[-1 * truncate_size : ]\n",
    "        synthetic_frames_truncated_audio = synthetic_frames[-1 * truncate_size : ]\n",
    "    \n",
    "    return truncated_audio , synthetic_frames_truncated_audio\n",
    "\n",
    "\n",
    "def get_new_preds(preds):\n",
    "    mm = []\n",
    "    for i in preds:\n",
    "        m = i.item()\n",
    "        if m <=.50:\n",
    "            mm.append(0)\n",
    "        else:\n",
    "            mm.append(1)\n",
    "    return mm\n",
    "            \n",
    "        \n",
    "def get_evaluations_ignoring_padding(targets, outputs, ignore_index):\n",
    "    targets = targets.flatten().long().cpu().numpy()\n",
    "    outputs = outputs.data.cpu().numpy()\n",
    "    indexes_to_remove = np.where(targets == ignore_index)\n",
    "    labels_nopad = np.delete(targets , indexes_to_remove)\n",
    "    preds_nopad = np.delete(outputs , indexes_to_remove)\n",
    "    accuracy_score = sklearn.metrics.accuracy_score(labels_nopad , preds_nopad)\n",
    "    recall_score = sklearn.metrics.recall_score(labels_nopad , preds_nopad)\n",
    "    precision_score = sklearn.metrics.precision_score(labels_nopad , preds_nopad)\n",
    "    dict1 = {'Accuracy' : accuracy_score , 'Recall' : recall_score , 'Precision' : precision_score}\n",
    "    return dict1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Creating_audio(Dataset):\n",
    "    def __init__(self, LibriSpeechData , VoicesDataSet, length_needed, padding, transform = None): #Libri, Voices)\n",
    "        self.length_needed = length_needed\n",
    "        self.padding = padding\n",
    "        self.Voices = Voices\n",
    "        self.transform = transform\n",
    "        if self.transform is None:\n",
    "            self.transform = torch.from_numpy\n",
    "        self.VoicesDataSet = VoicesDataSet(Voices)\n",
    "        self.LibriSpeechData = LibriSpeechData(df)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.VoicesDataSet)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        a , b = self.VoicesDataSet[index] # indexing for the voices \n",
    "        vad = VAD(a.numpy(), sr = 8000, theshold = .30)\n",
    "        zero_runs_intervals = zero_runs_full(vad)\n",
    "        sorted_silences = sorted(zero_runs_intervals,  key = lambda arr :  arr[1] - arr[0], reverse = True)[:2]\n",
    "            # Can make a threshold here of what the requirement of length is for a silent window (eg : 4000)\n",
    "            #If there is only one silence interval, then no need for the energy loop.\n",
    "        max_energy = 1000\n",
    "        start = 0\n",
    "        end = 0\n",
    "        for interval in sorted_silences:\n",
    "            energy_amount = statistics.mean(get_energy(a.numpy()[interval[0] : interval[1]]))\n",
    "            if energy_amount <= max_energy:\n",
    "                max_energy = energy_amount\n",
    "                start = interval[0]\n",
    "                end = interval[1] - 1\n",
    "\n",
    "        Voices_Silent_interval = list([start , end ])\n",
    "\n",
    "        full_Voices = a.numpy()\n",
    "\n",
    "        Flag = True\n",
    "        data = self.LibriSpeechData\n",
    "    \n",
    "        while Flag:\n",
    "            y, c  = data[random.choice(data.df.index)]\n",
    "            vad = VAD(y.numpy(), sr = 8000, theshold = .30)\n",
    "            interval_ones = one_runs(vad)\n",
    "            interval_zeros = zero_runs_full(vad) # Reverse = False for the short Libri Segments, True for the long Libri Segments                             \n",
    "            sorted_intervals = sorted(interval_ones, key = lambda arr : arr[1] - arr[0], reverse = False)\n",
    "            new_intervals = [i for i in sorted_intervals if 40000 not in i and 0 not in i and i[1] - i[0]>=self.length_needed]\n",
    "            if new_intervals:\n",
    "                intervals_passed = []\n",
    "                selected_interval=0\n",
    "                for i in new_intervals:\n",
    "                    start_left = interval_zeros[np.where(interval_zeros==i[0])[0]][0][0]\n",
    "                    end_left = interval_zeros[np.where(interval_zeros==i[0])[0]][0][1] - 1\n",
    "                    start_right = interval_zeros[np.where(interval_zeros==i[1] + 1)[0]][0][0]\n",
    "                    end_right = interval_zeros[np.where(interval_zeros==i[1] + 1)[0]][0][1] - 1\n",
    "                    if end_left - start_left >=self.padding and end_right - start_right >=self.padding:\n",
    "                        full_interval = list([i[0] - (end_left - start_left) , i[1] + (end_right - start_right)])\n",
    "                        #if full_interval[1] - full_interval[0]<=22000:\n",
    "                            #need this length requirement to make sure LibriSpeech does not dominate\n",
    "                        intervals_passed.append(full_interval)\n",
    "                if intervals_passed:\n",
    "                    r_interval = random.choice(intervals_passed)\n",
    "                    Libri_Speech = y.numpy()[r_interval[0] : r_interval[1]]\n",
    "                    Flag = False\n",
    "\n",
    "        new_audio = make_new_audio(full_Voices, Libri_Speech, Voices_Silent_interval[0], Voices_Silent_interval[1], Voices_Silent_interval)\n",
    "\n",
    "\n",
    "        synthetic_frames = audio_frames(full_Voices, Libri_Speech, Voices_Silent_interval[0], Voices_Silent_interval[1], Voices_Silent_interval)\n",
    "\n",
    "        new_audio , synthetic_frames = truncate_audio_variable_length(new_audio, synthetic_frames, upper = 28000, lower=None)\n",
    "        \n",
    "        new_audio = self.transform(normalizer(new_audio))\n",
    "        \n",
    "        synthetic_frames = self.transform(synthetic_frames)\n",
    "\n",
    "\n",
    "        return new_audio.float() , synthetic_frames # squeeze it for the interval [ ] task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import sklearn\n",
    "warnings.filterwarnings(\"ignore\", category=sklearn.exceptions.UndefinedMetricWarning)\n",
    "warnings.filterwarnings(\"ignore\", category = RuntimeWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = Creating_audio(LibriSpeechData, VoicesDataSet, length_needed = 3000, padding = 700,\n",
    "                   transform = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp = VoicesDataSet(Voices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = v[random.choice(mp.df.index)] # divide by zero in double scalars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fca403df5c0>]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(sample[0].numpy())\n",
    "plt.plot(sample[1].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train / Test Split \n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "batch_size = 24\n",
    "validation_split = .2\n",
    "shuffle_dataset = True\n",
    "random_seed= 42\n",
    "dataset_size = len(v) # len of the voices dataset \n",
    "indices = list(range(dataset_size))\n",
    "split = int(np.floor(validation_split * dataset_size))\n",
    "if shuffle_dataset :\n",
    "    np.random.seed(random_seed) # These will perform what is assigned to validation and what is \n",
    "                        #assigned to training\n",
    "    np.random.shuffle(indices)\n",
    "# grab indices \n",
    "train_indices, val_indices = indices[split:], indices[:split]\n",
    "\n",
    "\n",
    "# Creating PT data samplers and loaders:\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "valid_sampler = SubsetRandomSampler(val_indices)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing default collate to pack data into tuple of variable-length tensors \n",
    "torch.utils.data.dataloader.default_collate = (lambda default_collate = torch.utils.data.dataloader.default_collate: lambda batch: batch if all(map(torch.is_tensor, batch)) and any([tensor.size() != batch[0].size() for tensor in batch]) else default_collate(batch))()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.utils.data.dataloader.default_collate = torch.utils.data.dataloader.default_collate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(v, batch_size=batch_size, \n",
    "                                           sampler=train_sampler, num_workers=4, drop_last = True)\n",
    "validation_loader = torch.utils.data.DataLoader(v, batch_size=batch_size,\n",
    "                                                sampler=valid_sampler, num_workers=4, drop_last = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "from torch.nn.utils.rnn import pad_packed_sequence\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch import device\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from datetime import datetime\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Variable_LSTM(nn.Module):\n",
    "    def __init__(self, input_size , hidden_size , num_layers, output_dim = 1 , batch_size = batch_size):\n",
    "        super(Variable_LSTM , self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first = True)\n",
    "        self.to_linear = nn.Linear(hidden_size , output_dim)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "    def init_hidden(self):\n",
    "        \n",
    "        #initializaing hidden state to zeros\n",
    "        \n",
    "        return (Variable(torch.zeros(self.num_layers, self.batch_size, self.hidden_size)),\n",
    "                Variable(torch.zeros(self.num_layers, self.batch_size, self.hidden_size)))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Pack the padded sequence here\n",
    "        packed_time_series = pack_padded_sequence(x, self.get_length_tensor(x), batch_first=True)\n",
    "        \n",
    "        # Feed the packed sequence here\n",
    "        \n",
    "        lstm_out , self.hidden = self.lstm(packed_time_series.float())\n",
    "        \n",
    "        # pad_packed_sequence to reshape the output after the lstm \n",
    "        \n",
    "        padded_output, output_lens = pad_packed_sequence(lstm_out, batch_first=True)\n",
    "        \n",
    "        #feed padded_output to linear layer\n",
    "        \n",
    "        seq = self.to_linear(padded_output)\n",
    "        \n",
    "        # Pass the entirety of the lstm output for the seq2seq predictions\n",
    "        #y_pred = self.linear(lstm_out[-1].view(self.batch_size, -1))\n",
    "        \n",
    "        pred = torch.sigmoid(seq)\n",
    "    \n",
    "        return pred\n",
    "    \n",
    "    def get_length_tensor(self, x):\n",
    "        lengths = torch.Tensor([len(i) for i in x])\n",
    "        lengths = torch.sort(lengths, descending = True)[0]\n",
    "        \n",
    "        return lengths\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_dim = 2, batch_size = batch_size):\n",
    "        super(LSTM , self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.batch_size = batch_size\n",
    "        self.rnn_state = None\n",
    "        self.lstm = nn.LSTM(self.input_size , self.hidden_size , self.num_layers, batch_first = True)\n",
    "        self.to_linear = nn.Linear(self.hidden_size, output_dim)\n",
    "    def repackage_rnn_state(self):\n",
    "\n",
    "        self.rnn_state = self._detach_rnn_state(self.rnn_state)\n",
    "\n",
    "    def _detach_rnn_state(self, h):\n",
    "        \n",
    "        if isinstance(h, torch.Tensor):\n",
    "            return h.detach()\n",
    "        else:\n",
    "            return tuple(self._detach_rnn_state(v) for v in h)\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        lstm_out, self.rnn_state = self.lstm(x , self.rnn_state)\n",
    "        logits = self.to_linear(lstm_out)\n",
    "        scores = F.softmax(logits, dim = 2)\n",
    "        return scores\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_batch(seq_batch, labels_batch, bptt):\n",
    "    \"\"\"\n",
    "    Split torch.tensor batch by bptt steps, \n",
    "    Split seqence dim by bptt\n",
    "    \"\"\"\n",
    "    seq_batch_splits = seq_batch.split(bptt,dim=1)\n",
    "    labels_batch_splits = labels_batch.split(bptt, dim=1)\n",
    "    return seq_batch_splits, labels_batch_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = LSTM(input_size = 1, hidden_size = 128, num_layers = 2 , batch_size = batch_size)\n",
    "network.to(device)\n",
    "num_epochs = 20\n",
    "criterion = nn.CrossEntropyLoss()#ignore_index = 2)\n",
    "optimizer = optim.Adam(network.parameters(), lr = 0.001)\n",
    "#scheduler = StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "clip = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "     \n",
    "    running_loss_train , accuracy_train, running_recall_train = 0 , 0 , 0\n",
    "    \n",
    "    network.train()\n",
    "    \n",
    "    for batch_idx, batch in enumerate(train_loader):   \n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        network.rnn_state = None\n",
    "        \n",
    "        time_series , labels = batch\n",
    "        new_time_series, new_labels = time_series.to(device), labels.to(device)\n",
    "        \n",
    "        bptt_loss = 0\n",
    "        bptt_recall = 0\n",
    "        \n",
    "        bptt_audio_chunks, bptt_label_chunks = split_batch(new_time_series, new_labels.long(),50)\n",
    "        \n",
    "        bptt_audio_chunks = torch.cat(bptt_audio_chunks,0).split(672*4, dim = 0)\n",
    "        \n",
    "        bptt_label_chunks = torch.cat(bptt_label_chunks,0).split(672*4, dim = 0)\n",
    "        \n",
    "        #shape is now torch.size([batch_size , time_steps, num_features])\n",
    "        #num_features will be 1 after we unsqueeze\n",
    "        #now we learn through the batch chunks\n",
    "        \n",
    "        for idx_chunk , audio_chunk in enumerate(bptt_audio_chunks):\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            preds = network(audio_chunk.unsqueeze(2))\n",
    "            preds = preds.view(preds.shape[0]*preds.shape[1], preds.shape[2])\n",
    "            \n",
    "            new_labels = bptt_label_chunks[idx_chunk]\n",
    "            \n",
    "            train_loss = criterion(preds, new_labels.flatten())\n",
    "            bptt_loss += train_loss.item() / new_time_series.shape[0]\n",
    "            \n",
    "            train_loss.backward()\n",
    "            \n",
    "            torch.nn.utils.clip_grad_norm_(network.parameters(),clip)\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            network.repackage_rnn_state()\n",
    "            \n",
    "            bptt_recall+=recall_score(new_labels.flatten().cpu(), torch.max(preds, 1)[1].cpu())\n",
    "            \n",
    "        if batch_idx%1==0:\n",
    "            print('Epoch : %d | BatchID : %d | Train Loss : %.3f | Recall : %.3f ' % \n",
    "                  (epoch , batch_idx, bptt_loss / len(bptt_audio_chunks) , \n",
    "                   bptt_recall / len(bptt_audio_chunks)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "val_loss_list, epoch_list , accuracy_list = [], [], []  # val_accuracy_list\n",
    "\n",
    "print(\"Training...\\n\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    #scheduler.step()\n",
    "    \n",
    "    print('Epoch: ', epoch)#'LR: ', scheduler.get_lr())\n",
    "    \n",
    "    print('\\n')\n",
    "    \n",
    "    running_loss_train , accuracy_train = 0 , 0\n",
    "    \n",
    "    network.hidden = network.init_hidden()\n",
    "    \n",
    "    for batch_idx, batch in enumerate(train_loader):\n",
    "        \n",
    "        #at every step, we send the input and target to the GPU\n",
    "        \n",
    "        time_series , labels = batch\n",
    "        \n",
    "        #paddings for batches and the labels\n",
    "        \n",
    "        new_time_series = pad_sequence([torch.Tensor(t.float()) for t in time_series]).t()\n",
    "        \n",
    "        new_labels = pad_sequence([torch.Tensor(t.float()) for t in labels], padding_value = 2).t()\n",
    "        \n",
    "        new_time_series, new_labels = new_time_series.to(device), new_labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # forward pass feeding the padded sequence\n",
    "        \n",
    "        preds = network(new_time_series.unsqueeze(2))\n",
    "        \n",
    "        preds = preds.view(preds.shape[0]*preds.shape[1], preds.shape[2])\n",
    "        \n",
    "        train_loss = criterion(preds, new_labels.flatten().long())\n",
    "        \n",
    "        train_loss.backward()\n",
    "        \n",
    "        # gradient clipping here just before the parameter updates, clips inplace\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(network.parameters(),clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss_train+=train_loss.detach().item()/new_time_series.shape[0]\n",
    "        \n",
    "        _ ,predictions = torch.max(preds,1)\n",
    "        \n",
    "        metrics = get_evaluations_ignoring_padding(new_labels, predictions, 2)\n",
    "        \n",
    "        accuracy_train+=metrics['Accuracy']\n",
    "        \n",
    "\n",
    "        if batch_idx%100==0:\n",
    "            print('Epoch : %d | BatchID : %d | Train Loss : %.3f | Train Acc : %.3f ' % \n",
    "                  (epoch , batch_idx, train_loss.item() / new_time_series.shape[0] ,\n",
    "                   metrics['Accuracy']))\n",
    "     \n",
    "   # ----------------- VALIDATION  ----------------- \n",
    "    \n",
    "    print(\"Validation....\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        network.eval()\n",
    "        \n",
    "        network.hidden = network.init_hidden() # not stateful\n",
    "        \n",
    "        running_loss_val, accuracy_val, recall_val , precision_val = 0 , 0 , 0 , 0\n",
    "        \n",
    "        for batch_idx_val, batch_val in enumerate(validation_loader):\n",
    "                                   \n",
    "            time_series_val , labels_val = batch_val\n",
    "            \n",
    "            new_time_series_val = pad_sequence([torch.Tensor(t.float()) for t in time_series_val]).t()\n",
    "            \n",
    "            new_labels_val = pad_sequence([torch.Tensor(t.float()) for t in labels_val], padding_value = 2).t()\n",
    "            \n",
    "            new_time_series_val = new_time_series_val.to(device)\n",
    "            \n",
    "            new_labels_val = new_labels_val.to(device)\n",
    "            \n",
    "            preds_val = network(new_time_series_val.unsqueeze(2))\n",
    "            \n",
    "            preds_val = preds_val.view(preds_val.shape[0]*preds_val.shape[1], preds_val.shape[2])\n",
    "            \n",
    "            val_loss = criterion(preds_val, new_labels_val.flatten().long())\n",
    "            \n",
    "            _ ,predictions_val = torch.max(preds_val,1)\n",
    "            \n",
    "            running_loss_val+=val_loss.detach().item() / new_time_series_val.shape[0]\n",
    "            \n",
    "            metrics_val = get_evaluations_ignoring_padding(new_labels_val , predictions_val , 2)\n",
    "            \n",
    "            accuracy_val+= metrics_val['Accuracy']\n",
    "            \n",
    "            recall_val+= metrics_val['Recall']\n",
    "            \n",
    "            precision_val+=metrics_val['Precision']\n",
    "            \n",
    "            \n",
    "        network.train()\n",
    "        \n",
    "        print('Epoch : %d | Train Loss : %.3f | Train Acc : %.3f | Val Loss : %.3f | Val Acc : %.3f | Recall : %.3f | Precision : %.3f' \n",
    "              % (\n",
    "                  epoch , \n",
    "                  running_loss_train / len(train_loader),\n",
    "                  accuracy_train / len(train_loader),\n",
    "                  running_loss_val / len(validation_loader),\n",
    "                  accuracy_val / len(validation_loader),\n",
    "                  recall_val / len(validation_loader),\n",
    "                  precision_val / len(validation_loader)\n",
    "              )\n",
    "          )\n",
    "                    \n",
    "    epoch_list.append(epoch)\n",
    "    val_loss_list.append(running_loss_val / len(validation_loader))\n",
    "    accuracy_list.append(accuracy_val / len(validation_loader))\n",
    "      \n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualization loss\n",
    "plt.plot(epoch_list, val_loss_list)\n",
    "plt.xlabel(\"# of epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"LSTM: Loss vs # epochs\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# visualization accuracy\n",
    "plt.plot(epoch_list, accuracy_list, color=\"red\")\n",
    "plt.xlabel(\"# of epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"LSTM: Accuracy vs # epochs\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(network.lstm)\n",
    "#print(network.lstm.weight_hh_l0)\n",
    "#print(network.lstm._all_weights)\n",
    "#print(network.to_linear.weight)\n",
    "torch.save(network.state_dict(), 'LSTM_parameters.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
