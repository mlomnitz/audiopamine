{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import librosa\n",
    "import os\n",
    "from skimage import io\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset\n",
    "import math\n",
    "import IPython.display as ipd\n",
    "\n",
    "## Reconstructing File Paths\n",
    "\n",
    "file_path = 'Datasets/LibriSpeech/train-clean-100.index.csv'\n",
    "\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "df.columns\n",
    "\n",
    "df.head()\n",
    "\n",
    "df.shape\n",
    "\n",
    "\n",
    "df.filepath[1]\n",
    "\n",
    "df.filepath[1][0:38]\n",
    "\n",
    "mine = '/home/amonajemi/amonajemi/Datasets'\n",
    "\n",
    "df['filepath'] = df['filepath'].str[37:]\n",
    "\n",
    "df['filepath'] = mine + df['filepath']\n",
    "\n",
    "df['filepath'].head()\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class LibriSpeechData(Dataset):\n",
    "    def __init__(self,df,transform = None, sampling_rate = 8000, offset=1.0, duration=5.0):\n",
    "        self.duration = duration\n",
    "        self.df = df[df.seconds > self.duration].reset_index()\n",
    "        self.audio = self.df['filepath']\n",
    "        self.identities=self.df['id']\n",
    "        self.transform = transform\n",
    "        self.sampling_rate = sampling_rate\n",
    "        if self.transform is None:\n",
    "            self.transform = torch.from_numpy # Call it as an object! Now transform is defined so \n",
    "                                                # we cn iterate through the batches. \n",
    "\n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        audio = self.audio[index]\n",
    "        self.seconds = self.df['seconds'][index]\n",
    "        self.offset = random.uniform(0,self.seconds - self.duration)\n",
    "        y,  sr  = librosa.load(audio, sr=self.sampling_rate, offset=self.offset, duration=self.duration)\n",
    "        # The y is the one-dimensional np.array\n",
    "        #that is the amplitude of the waveform at sample t\n",
    "        \n",
    "        #the sr is the number of samples per second of a\n",
    "        #time series.\n",
    "    \n",
    "        time_series = self.transform(y)\n",
    "        \n",
    "        return time_series, torch.tensor(self.identities[index])        \n",
    "\n",
    "df.index # Our filtering of the dataset kept the original index. This is why you see 28437 here but the\n",
    "#overall length of indexes is indeed 26504 \n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa.display\n",
    "\n",
    "\n",
    "    \n",
    "def energy_plot(a, hop_length = 256, frame_length = 2048):\n",
    "    energy = np.array([\n",
    "        sum(abs(a.numpy()[i:i+frame_length]**2))\n",
    "        for i in range(0, len(a.numpy()), hop_length)])\n",
    "    return plt.plot(energy / energy.max()), a.min(),a.max()\n",
    "\n",
    "## Energy Detection \n",
    "\n",
    "def energy_values(a, hop_length = 256, frame_length = 2048):\n",
    "    energy = np.array([\n",
    "        sum(abs(a.numpy()[i:i+frame_length]**2))\n",
    "        for i in range(0, len(a.numpy()), hop_length)])\n",
    "    return energy\n",
    "def energy_values(a, hop_length = 256, frame_length = 2048):\n",
    "    energy = np.array([\n",
    "        sum(abs(a[i:i+frame_length]**2))\n",
    "        for i in range(0, len(a), hop_length)])\n",
    "    return energy\n",
    "\n",
    "get_energy = energy_values\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "Voices = pd.read_csv(\"/home/amonajemi/amonajemi/Datasets/VOiCES/VOiCES-room-1.index.csv\")\n",
    "\n",
    "Voices.columns\n",
    "\n",
    "Voices['filepath'][40000]\n",
    "\n",
    "Voices['filepath'][1][22:]\n",
    "\n",
    "file_e = '/home/amonajemi/amonajemi/Datasets/VOiCES/Volumes/'\n",
    "\n",
    "Voices['filepath'] = Voices['filepath'].str[22:]\n",
    "\n",
    "Voices['filepath'] = file_e + Voices['filepath']\n",
    "    \n",
    "\n",
    "Voices = Voices[~Voices.filepath.str.contains(\"tele\")]\n",
    "\n",
    "len(Voices)\n",
    "\n",
    "Voices.index = range(0,len(Voices))\n",
    "\n",
    "Voices = Voices[(Voices.filepath.str.contains(\"sp_3549\") & Voices.filepath.str.contains(\"none\"))|(Voices.filepath.str.contains(\"sp_6241\") & Voices.filepath.str.contains(\"none\")) | (Voices.filepath.str.contains(\"sp_1212\") & Voices.filepath.str.contains(\"musi\")) | (Voices.filepath.str.contains(\"sp_6241\") & Voices.filepath.str.contains(\"musi\"))]\n",
    "\n",
    "Voices.index = range(0,len(Voices))\n",
    "\n",
    "\n",
    "\n",
    "class VoicesDataSet(Dataset):\n",
    "    def __init__(self,df,transform = None, sampling_rate = 8000, offset=1.0, duration=5.0):\n",
    "        self.duration = duration\n",
    "        self.df = df[df.seconds > self.duration].reset_index()\n",
    "        self.audio = self.df['filepath']\n",
    "        self.identities=self.df['id']\n",
    "        self.transform = transform\n",
    "        self.sampling_rate = sampling_rate\n",
    "        if self.transform is None:\n",
    "            self.transform = torch.from_numpy \n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        audio = self.audio[index]\n",
    "        self.seconds = self.df['seconds'][index]\n",
    "        self.offset = random.uniform(0,self.seconds - self.duration)\n",
    "        y,  sr  = librosa.load(audio, sr=self.sampling_rate, offset=self.offset, duration=self.duration)\n",
    "    \n",
    "        time_series = self.transform(y)\n",
    "        \n",
    "        return time_series, torch.tensor(self.identities[index])\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fade In / Fade Out\n",
    "import statistics\n",
    "def fading_in(length):\n",
    "    fading_in =np.linspace(0, 1, length)\n",
    "    #fading_in[int(length*.50):] = 1\n",
    "    return fading_in\n",
    "\n",
    "def fading_out(length):\n",
    "    fading_out = np.linspace(0,1, length)[::-1]\n",
    "    #fading_out[:int(length * .50)] = 1\n",
    "    return fading_out\n",
    "\n",
    "\n",
    "\n",
    "# VAD\n",
    "\n",
    "def zero_runs(a):\n",
    "    # Create an array that is 1 where a is 0, and pad each end with an extra 0.\n",
    "    iszero = np.concatenate(([0], np.equal(a, 0).view(np.int8), [0]))\n",
    "    absdiff = np.abs(np.diff(iszero))\n",
    "    # Runs start and end where absdiff is 1.\n",
    "    ranges = np.where(absdiff == 1)[0].reshape(-1, 2)\n",
    "    greatest = 0\n",
    "    start = 0       # For Voices change this to the max interval again like below\n",
    "    end = 0\n",
    "    for interval in ranges:\n",
    "        diff = interval[1] - interval[0]\n",
    "        if diff > greatest:\n",
    "            greatest = diff\n",
    "            start = interval[0]\n",
    "            end = interval[1]\n",
    "    return list([start, end - 1])\n",
    "\n",
    "def zero_runs_full(a):\n",
    "    # Create an array that is 1 where a is 0, and pad each end with an extra 0.\n",
    "    iszero = np.concatenate(([0], np.equal(a, 0).view(np.int8), [0]))\n",
    "    absdiff = np.abs(np.diff(iszero))\n",
    "    # Runs start and end where absdiff is 1.\n",
    "    ranges = np.where(absdiff == 1)[0].reshape(-1, 2)\n",
    "    return ranges\n",
    "\n",
    "# Step 1 : Take an instance of the Voices Class\n",
    "def Voices_data_Speech(dataset_class, _data):\n",
    "    data = dataset_class(_data)\n",
    "    #global a\n",
    "    a,b = data[random.choice(_data.index)]\n",
    "    vad = VAD(a.numpy(), sr = 8000, theshold = .30)\n",
    "    zero_runs_intervals = zero_runs_full(vad)\n",
    "    sorted_silences = sorted(zero_runs_intervals,  key = lambda arr :  arr[1] - arr[0], reverse = True)[:2]\n",
    "    max_energy = 1000\n",
    "    start = 0\n",
    "    end = 0\n",
    "    for interval in sorted_silences:\n",
    "        energy_amount = statistics.mean(get_energy(a.numpy()[interval[0] : interval[1]]))\n",
    "        if energy_amount <= max_energy:\n",
    "            max_energy = energy_amount\n",
    "            start = interval[0]\n",
    "            end = interval[1] - 1\n",
    "    #plt.plot(vad)\n",
    "    #plt.plot(a.numpy())\n",
    "    return list([start, end])\n",
    "\n",
    "def one_runs(a):\n",
    "    # Create an array that is 1 where a is 0, and pad each end with an extra 0.\n",
    "    iszero = np.concatenate(([0], np.equal(a, 0).view(np.int8), [0]))\n",
    "    absdiff = np.abs(np.diff(iszero))\n",
    "    # Runs start and end where absdiff is 1.\n",
    "    ranges = np.where(absdiff == 1)[0].reshape(-1, 2)\n",
    "    interval = []\n",
    "    for i in range(len(ranges) - 1):\n",
    "        start_one = ranges[i][1]\n",
    "        upper = i+1\n",
    "        end_one = ranges[upper][0]-1\n",
    "        interval.append(start_one)\n",
    "        interval.append(end_one)\n",
    "    ranges_ones = np.array(interval).reshape(-1,2)\n",
    "    \n",
    "    return ranges_ones \n",
    "    \n",
    "\n",
    "\n",
    "def make_new_audio(Voices_data, Lib_data, silent_begin, silent_end, silent_interval):\n",
    "    if silent_begin is not 0:\n",
    "        sample_audio = Voices_data[:int(silent_begin)]\n",
    "      \n",
    "        \n",
    "        Voices_Segment1 = Voices_data[:int(silent_begin)] \n",
    "    \n",
    "    half1_silent = Voices_data[int(silent_begin) : int(statistics.mean(silent_interval))]\n",
    "    fade_out_half1 = half1_silent * fading_out(len(half1_silent))\n",
    "    \n",
    "    Libri_audio_half1 = Lib_data[:int(math.floor(len(Lib_data) / 2))]\n",
    "    Libri_audio_half1 = Libri_audio_half1\n",
    "    \n",
    "    Libri_audio_half2 = Lib_data[int(math.floor(len(Lib_data) / 2)):]\n",
    "    Libri_audio_half2 = Libri_audio_half2\n",
    "    \n",
    "    Libri_full = np.concatenate((Libri_audio_half1 , Libri_audio_half2))\n",
    "    \n",
    "    half2_silent = Voices_data[int(statistics.mean(silent_interval)):int(silent_end)]\n",
    "    fade_in_half2 = half2_silent* fading_in(len(half2_silent)) # Do not do fading here!\n",
    "    \n",
    "    \n",
    "    sample_audio1 = Voices_data[int(silent_end):]\n",
    "\n",
    "    Voices_Segment2 = Voices_data[int(silent_end):]\n",
    "    if Voices_Segment1 is not None: # Put this not .all()\n",
    "        new_audio = np.concatenate((Voices_Segment1, fade_out_half1,Libri_full, fade_in_half2, Voices_Segment2))\n",
    "    else:\n",
    "        new_audio = np.concatenate((fade_out_half1, Libri_full, fade_in_half2, Voices_Segment2))\n",
    "        \n",
    "    return new_audio\n",
    "\n",
    "def audio_frames(Voices_data, Lib_data, silent_begin, silent_end, silent_interval):\n",
    "    # create zeros and ones for frames \n",
    "    _Voices_seg1 = np.zeros(len(Voices_data[:int(silent_begin)]))\n",
    "    _silent1 = np.zeros(len(Voices_data[int(silent_begin) : int(statistics.mean(silent_interval))]))\n",
    "    _Libri = np.ones(len(Lib_data))\n",
    "    _silent2 = np.zeros(len(Voices_data[int(statistics.mean(silent_interval)) : int(silent_end)]))\n",
    "    _Voices_seg2 = np.zeros(len(Voices_data[int(silent_end):]))\n",
    "    frames = np.concatenate((_Voices_seg1, _silent1, _Libri, _silent2, _Voices_seg2))\n",
    "    #plt.plot(frames)\n",
    "    return frames\n",
    "    \n",
    "\n",
    "def normalizer(data):\n",
    "    series = Series(data)\n",
    "    values = series.values\n",
    "    values = values.reshape((len(values), 1))\n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "    scaler = scaler.fit(values)\n",
    "    normalized = scaler.transform(values)\n",
    "    normalized = normalized.reshape(-1)\n",
    "    return normalized\n",
    "    \n",
    "def truncate_audio(new_audio, synthetic_frames, truncate_size) : \n",
    "    right_prop_libri = sum(synthetic_frames[0 : truncate_size]) / len(synthetic_frames[0 : truncate_size])\n",
    "\n",
    "    left_prop_libri = sum(synthetic_frames[-1 * truncate_size : ]) / len(synthetic_frames[-1*truncate_size:])\n",
    "    \n",
    "    if abs(right_prop_libri - 0.30) <= abs(left_prop_libri - 0.30) and abs(right_prop_libri - 0.30)<=0.15:\n",
    "        truncated_audio = new_audio[0 : truncate_size]\n",
    "        synthetic_frames_truncated_audio = synthetic_frames[0:truncate_size]\n",
    "        \n",
    "    else:\n",
    "        truncated_audio = new_audio[-1 * truncate_size : ]\n",
    "        synthetic_frames_truncated_audio = synthetic_frames[-1 * truncate_size : ]\n",
    "    \n",
    "    return truncated_audio , synthetic_frames_truncated_audio\n",
    "\n",
    "\n",
    "def get_dist_libri(n_iter):\n",
    "    ratio = []\n",
    "    for i in range(n_iter):\n",
    "        sample = v[random.choice(Voices.index)]\n",
    "        if len(sample)==3:\n",
    "            ratio.append(sum(sample[2]) / len(sample[2]))\n",
    "    return ratio\n",
    "\n",
    "def truncate_until_prop(new_audio , synthetic_frames , truncated_length, prop_min , prop_max):\n",
    "    audio_length = len(new_audio)\n",
    "    end_point = audio_length - truncated_length\n",
    "    Flag = True\n",
    "    while Flag:\n",
    "        rand_index = random.randint(0,end_point)\n",
    "        prop = sum(synthetic_frames[rand_index : rand_index + truncated_length]) / truncated_length\n",
    "        if prop_min <=prop<=prop_max:\n",
    "            Flag = False\n",
    "            new_audio = new_audio[rand_index : rand_index + truncated_length]\n",
    "            synthetic_regions = synthetic_frames[rand_index : rand_index + truncated_length]\n",
    "    return new_audio, prop, synthetic_regions\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train / Test Split \n",
    "\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import numpy as np\n",
    "from random import sample\n",
    "from pandas import Series\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "validation_split = .2\n",
    "shuffle_dataset = True\n",
    "random_seed= 42\n",
    "data = VoicesDataSet(Voices)\n",
    "dataset_size = len(data) # len of the voices dataset \n",
    "indices = list(range(dataset_size))\n",
    "split = int(np.floor(validation_split * dataset_size))\n",
    "if shuffle_dataset :\n",
    "    np.random.seed(random_seed) # These will perform what is assigned to validation and what is \n",
    "                        #assigned to training\n",
    "    np.random.shuffle(indices)\n",
    "# grab indices \n",
    "train_indices, val_indices = indices[split:], indices[:split]\n",
    "\n",
    "\n",
    "# Creating PT data samplers and loaders:\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "valid_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "\n",
    "\n",
    "indices_real_train = sample(train_indices , len(train_indices) // 2)\n",
    "indices_fake_train = [i for i in train_indices if i not in indices_real_train]\n",
    "indices_real_val = sample(val_indices , len(val_indices) // 2)\n",
    "indices_fake_val = [i for i in val_indices if i not in indices_real_val]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Creating_audio(Dataset):\n",
    "    def __init__(self, LibriSpeechData , VoicesDataSet, length_needed, padding,indices_real_train, \n",
    "                 indices_fake_train, indices_real_val, indices_fake_val, transform = None): #Libri, Voices)\n",
    "        self.length_needed = length_needed\n",
    "        self.padding = padding\n",
    "        self.indices_real_train = indices_real_train\n",
    "        self.indices_fake_train = indices_fake_train\n",
    "        self.indices_real_val = indices_real_val\n",
    "        self.indices_fake_val = indices_fake_val\n",
    "        self.transform = transform\n",
    "        if self.transform is None:\n",
    "            self.transform = torch.from_numpy\n",
    "        self.LibriSpeechData = LibriSpeechData(df) # Would need to get data here\n",
    "        self.VoicesDataSet = VoicesDataSet(Voices)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.VoicesDataSet) # self.Voices.shape[0] # we cut off the Voices, may need to change this\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        if index in self.indices_real_train or index in self.indices_real_val: \n",
    "            \n",
    "            a, b = self.VoicesDataSet[index]\n",
    "            a = a[0:32000]\n",
    "            a = self.transform(normalizer(a))\n",
    "            return a , 0\n",
    "            \n",
    "        else:\n",
    "            a , b = self.VoicesDataSet[index]\n",
    "            vad = VAD(a.numpy(), sr = 8000, theshold = .30)\n",
    "            zero_runs_intervals = zero_runs_full(vad)\n",
    "            sorted_silences = sorted(zero_runs_intervals,  key = lambda arr :  arr[1] - arr[0], reverse = True)[:2]\n",
    "                # Can make a threshold here of what the requirement of length is for a silent window (eg : 4000)\n",
    "                #If there is only one silence interval, then no need for the energy loop.\n",
    "            max_energy = 1000\n",
    "            start = 0\n",
    "            end = 0\n",
    "            for interval in sorted_silences:\n",
    "                energy_amount = statistics.mean(get_energy(a.numpy()[interval[0] : interval[1]]))\n",
    "                if energy_amount <= max_energy:\n",
    "                    max_energy = energy_amount\n",
    "                    start = interval[0]\n",
    "                    end = interval[1] - 1\n",
    "\n",
    "            Voices_Silent_interval = list([start , end])\n",
    "\n",
    "            full_Voices = a.numpy()\n",
    "\n",
    "            data = self.LibriSpeechData\n",
    "            Flag = True\n",
    "            while Flag:\n",
    "                y, c  = data[random.choice(data.df.index)]\n",
    "                vad = VAD(y.numpy(), sr = 8000, theshold = .30)\n",
    "                interval_ones = one_runs(vad)\n",
    "                interval_zeros = zero_runs_full(vad) # Reverse = False for the short Libri Segments, True for the long Libri Segments                             \n",
    "                sorted_intervals = sorted(interval_ones, key = lambda arr : arr[1] - arr[0], reverse = False)\n",
    "                new_intervals = [i for i in sorted_intervals if 40000 not in i and 0 not in i and i[1] - i[0]>=self.length_needed]\n",
    "                selected_interval=0\n",
    "                intervals_passed = []\n",
    "                if new_intervals:\n",
    "                    for i in new_intervals:\n",
    "                        start_left = interval_zeros[np.where(interval_zeros==i[0])[0]][0][0]\n",
    "                        end_left = interval_zeros[np.where(interval_zeros==i[0])[0]][0][1] - 1\n",
    "                        start_right = interval_zeros[np.where(interval_zeros==i[1] + 1)[0]][0][0]\n",
    "                        end_right = interval_zeros[np.where(interval_zeros==i[1] + 1)[0]][0][1] - 1\n",
    "                        if end_left - start_left >=self.padding and end_right - start_right >=self.padding:\n",
    "                            full_interval = list([i[0] - (end_left - start_left) , i[1] + (end_right - start_right)])\n",
    "                            if full_interval[1] - full_interval[0]<=22000:\n",
    "                               #Putting a length requirement on LibriSpeech\n",
    "                                intervals_passed.append(full_interval)\n",
    "                    if intervals_passed:\n",
    "                        r_interval = random.choice(intervals_passed)\n",
    "                        Libri_Speech = y.numpy()[r_interval[0] : r_interval[1]]\n",
    "                        Flag = False\n",
    "\n",
    "            new_audio = make_new_audio(full_Voices, Libri_Speech, Voices_Silent_interval[0], Voices_Silent_interval[1], Voices_Silent_interval)\n",
    "\n",
    "\n",
    "            synthetic_frames = audio_frames(full_Voices, Libri_Speech, Voices_Silent_interval[0], Voices_Silent_interval[1], Voices_Silent_interval)\n",
    "                \n",
    "            new_audio , prop, synthetic_regions = truncate_until_prop(new_audio , synthetic_frames , 32000 , 0.05 , 0.45)\n",
    "\n",
    "            return self.transform(normalizer(new_audio)).float() , 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import sklearn\n",
    "warnings.filterwarnings(\"ignore\", category=sklearn.exceptions.UndefinedMetricWarning)\n",
    "warnings.filterwarnings(\"ignore\", category = RuntimeWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = Creating_audio(LibriSpeechData, VoicesDataSet, length_needed = 3000, padding = 700,\n",
    "                   indices_real_train = indices_real_train , indices_fake_train = indices_fake_train, \n",
    "                   indices_real_val = indices_real_val, indices_fake_val = indices_fake_val, transform = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "32000\n"
     ]
    }
   ],
   "source": [
    "pp=VoicesDataSet(Voices)\n",
    "pp.df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = v[random.choice(pp.df.index)]\n",
    "plt.plot(sample[0].numpy())\n",
    "plt.plot(sample[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipd.Audio(sample[0].numpy() , rate = 8000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjusting collate to pack data into tuple of tensors - accepts variable length inputs \n",
    "\n",
    "torch.utils.data.dataloader.default_collate = (lambda default_collate = torch.utils.data.dataloader.default_collate: lambda batch: batch if all(map(torch.is_tensor, batch)) and any([tensor.size() != batch[0].size() for tensor in batch]) else default_collate(batch))()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.utils.data.dataloader.default_collate = torch.utils.data.dataloader.default_collate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 24\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(v, batch_size=batch_size, \n",
    "                                           sampler=train_sampler, num_workers=4, drop_last = True)\n",
    "validation_loader = torch.utils.data.DataLoader(v, batch_size=batch_size,\n",
    "                                                sampler=valid_sampler, num_workers=4, drop_last = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series , labels = batch\n",
    "print(time_series)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch import device\n",
    "from torch import optim\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(1, 128, 80, 4)\n",
    "        self.bn1 = nn.BatchNorm1d(128)\n",
    "        self.pool1 = nn.MaxPool1d(4)\n",
    "        self.conv2 = nn.Conv1d(128, 128, 3)\n",
    "        self.bn2 = nn.BatchNorm1d(128)\n",
    "        self.pool2 = nn.MaxPool1d(4)\n",
    "        self.conv3 = nn.Conv1d(128, 256, 3)\n",
    "        self.bn3 = nn.BatchNorm1d(256)\n",
    "        self.pool3 = nn.MaxPool1d(4)\n",
    "        self.conv4 = nn.Conv1d(256, 512, 3)\n",
    "        self.bn4 = nn.BatchNorm1d(512)\n",
    "        self.pool4 = nn.MaxPool1d(4)\n",
    "        #self.avgPool = nn.AvgPool1d(30) #input should be 512x30 so this outputs a 512x1\n",
    "        self.fc1 = nn.Linear(512*30, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.shape[0], 1,-1)\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(self.bn1(x))\n",
    "        x = self.pool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(self.bn2(x))\n",
    "        x = self.pool2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = F.relu(self.bn3(x))\n",
    "        x = self.pool3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = F.relu(self.bn4(x))\n",
    "        x = self.pool4(x)\n",
    "#       x = self.avgPool(x)\n",
    "        x = x.view(batch_size, -1)\n",
    "        #x = x.permute(0, 2, 1) #change the 512x1 to 1x512\n",
    "        x = self.fc1(x)\n",
    "        #x = F.softmax(x, dim=1)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_dim = 1, batch_size = batch_size):\n",
    "        super(CRNN , self).__init__()\n",
    "        self.conv1 = nn.Conv1d(1, 64, 80, 4)\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.pool1= nn.MaxPool1d(4)\n",
    "        self.conv2 = nn.Conv1d(64, 64, 3)\n",
    "        self.bn2 = nn.BatchNorm1d(64)\n",
    "        self.pool2 = nn.MaxPool1d(4)\n",
    "        self.conv3 = nn.Conv1d(64, 128, 3)\n",
    "        self.bn3 = nn.BatchNorm1d(128)\n",
    "        self.conv4 = nn.Conv1d(128, 128, 3)\n",
    "        self.bn4 = nn.BatchNorm1d(128)\n",
    "        self.pool3 = nn.MaxPool1d(4)\n",
    "        self.conv5 = nn.Conv1d(128, 256, 3)\n",
    "        self.bn5 = nn.BatchNorm1d(256)\n",
    "        self.conv6 = nn.Conv1d(256, 256, 3)\n",
    "        self.bn6 = nn.BatchNorm1d(256)\n",
    "        self.pool4 = nn.MaxPool1d(4)\n",
    "        self.conv7 = nn.Conv1d(256, 512 , 3)\n",
    "        self.bn7 = nn.BatchNorm1d(512)\n",
    "        self.conv8 = nn.Conv1d(512, 512, 3)\n",
    "        self.pool5 = nn.MaxPool1d(4)\n",
    "        #shape batch size x 512 x 6\n",
    "        self.num_layers = num_layers\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.batch_size = batch_size\n",
    "        self.output_dim = output_dim\n",
    "        #input size is 6 features time step\n",
    "        #batch_size x 512 x 6\n",
    "        self.lstm = nn.LSTM(self.input_size , self.hidden_size , self.num_layers, batch_first = True)\n",
    "        self.to_linear = nn.Linear(self.hidden_size , self.output_dim)\n",
    "    def init_hidden(self):\n",
    "        return (Variable(torch.zeros(self.num_layers, self.batch_size, self.hidden_size)),\n",
    "                Variable(torch.zeros(self.num_layers, self.batch_size, self.hidden_size)))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.view(x.shape[0], 1,-1)\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(self.bn1(x))\n",
    "        x = self.pool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(self.bn2(x))\n",
    "        x = self.pool2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = F.relu(self.bn3(x))\n",
    "        x = self.conv4(x)\n",
    "        x = F.relu(self.bn4(x))\n",
    "        x = self.pool3(x)\n",
    "        x = self.conv5(x)\n",
    "        x = F.relu(self.bn5(x))\n",
    "        x = self.conv6(x)\n",
    "        x = F.relu(self.bn6(x))\n",
    "        x = self.pool4(x)\n",
    "        x = self.conv7(x)\n",
    "        x = F.relu(self.bn7(x))\n",
    "        x = self.conv8(x)\n",
    "        x = F.relu(self.pool5(x))\n",
    "        #x = x.view(batch_size , -1, 1)\n",
    "        lstm_out, hidden = self.lstm(x)\n",
    "        logits = self.to_linear(lstm_out)\n",
    "        scores = torch.sigmoid(logits)\n",
    "        return scores\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN8(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN8, self).__init__()\n",
    "\n",
    "        # 32000 x 1\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv1d(1, 128, kernel_size=3, stride=3, padding=0),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU())\n",
    "        # 10666 x 128\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv1d(128, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(3, stride=3))\n",
    "        # 3555 x 128\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv1d(128, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(3,stride=3))\n",
    "        # 1185 x 128\n",
    "        self.conv4 = nn.Sequential(\n",
    "            nn.Conv1d(128, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(3,stride=3))\n",
    "        # 395 x 256\n",
    "        self.conv5 = nn.Sequential(\n",
    "            nn.Conv1d(256, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(3,stride=3))\n",
    "        # 131 x 256\n",
    "        self.conv6 = nn.Sequential(\n",
    "            nn.Conv1d(256, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(3,stride=3))\n",
    "        # 43 x 256\n",
    "        self.conv7 = nn.Sequential(\n",
    "            nn.Conv1d(256, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(3,stride=3))\n",
    "        # 14 x 256\n",
    "        self.conv8 = nn.Sequential(\n",
    "            nn.Conv1d(256, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(3,stride=3))\n",
    "        # 4 x 256\n",
    "        self.fc = nn.Linear(4 * 256, 1) # 512\n",
    "\n",
    "    def forward(self, x):\n",
    "        # input x : 24 x 32000 x 1\n",
    "        # expected conv1d input : minibatch_size x num_channel x width\n",
    "        x = x.view(x.shape[0], 1,-1)\n",
    "        # x : 24 x 1 x 32000 \n",
    "        out = self.conv1(x)\n",
    "        out = self.conv2(out)\n",
    "        out = self.conv3(out)\n",
    "        out = self.conv4(out)\n",
    "        out = self.conv5(out)\n",
    "        out = self.conv6(out)\n",
    "        out = self.conv7(out)\n",
    "        out = self.conv8(out)\n",
    "        #out = self.conv9(out)\n",
    "        #out = self.conv10(out) \n",
    "        out = out.view(x.shape[0],-1)\n",
    "        logit = self.fc(out)\n",
    "        scores = torch.sigmoid(logit)\n",
    "\n",
    "        return scores\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = CNN()\n",
    "model.to(device)\n",
    "model_CRNN = CRNN()\n",
    "mode_CRNN.to(device)\n",
    "model_CNN8 = CNN8()\n",
    "model_CNN8.to(device)\n",
    "model_CRNN = CRNN(input_size = 6, hidden_size = 128, num_layers = 2)\n",
    "model_CRNN.to(device)\n",
    "optimizer_CNN8 = optim.Adam(model_CNN8.parameters(), lr = 0.001)\n",
    "optimizer_CRNN = optim.Adam(model_CRNN.parameters(), lr = 0.001)\n",
    "optimizer_CNN = optim.Adam(model_CNN.parameters(), lr = 0.001)\n",
    "#criterion = nn.CrossEntropyLoss()\n",
    "criterion = nn.BCELoss()\n",
    "num_epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, recall_score, precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_preds(preds):\n",
    "    new_preds = []\n",
    "    for i in preds:\n",
    "        if i <=.50:\n",
    "            new_preds.append(0)\n",
    "        else:\n",
    "            new_preds.append(1)\n",
    "    return torch.tensor(new_preds)\n",
    "def get_accuracy(preds , labels, batch_size):\n",
    "    return preds.argmax(dim = 1).eq(labels).sum().item() / batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#val_loss_list_CNN8 , epoch_list_CNN8, accuracy_list_CNN8 = [], [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/torch/nn/functional.py:2016: UserWarning: Using a target size (torch.Size([24])) that is different to the input size (torch.Size([24, 1])) is deprecated. Please ensure they have the same size.\n",
      "  \"Please ensure they have the same size.\".format(target.size(), input.size()))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 0 | Batch : 0 | Train Loss : 0.030 | Train Acc : 0.542 | Train Recall : 0.786 \n",
      "Epoch : 0 | Batch : 50 | Train Loss : 0.027 | Train Acc : 0.625 | Train Recall : 0.462 \n",
      "Epoch : 0 | Batch : 100 | Train Loss : 0.019 | Train Acc : 0.708 | Train Recall : 0.583 \n",
      "Epoch : 0 | Batch : 150 | Train Loss : 0.018 | Train Acc : 0.875 | Train Recall : 0.818 \n",
      "Epoch : 0 | Batch : 200 | Train Loss : 0.025 | Train Acc : 0.625 | Train Recall : 0.429 \n",
      "Epoch : 0 | Batch : 250 | Train Loss : 0.015 | Train Acc : 0.958 | Train Recall : 0.900 \n",
      "Epoch : 0 | Batch : 300 | Train Loss : 0.010 | Train Acc : 0.917 | Train Recall : 0.818 \n",
      "Epoch : 0 | Batch : 350 | Train Loss : 0.006 | Train Acc : 1.000 | Train Recall : 1.000 \n",
      "Epoch : 0 | Batch : 400 | Train Loss : 0.013 | Train Acc : 0.792 | Train Recall : 0.818 \n",
      "Epoch : 0 | Batch : 450 | Train Loss : 0.007 | Train Acc : 1.000 | Train Recall : 1.000 \n",
      "Epoch : 0 | Batch : 500 | Train Loss : 0.006 | Train Acc : 1.000 | Train Recall : 1.000 \n",
      "Epoch : 0 | Batch : 550 | Train Loss : 0.013 | Train Acc : 0.833 | Train Recall : 0.778 \n",
      "Epoch : 0 | Batch : 600 | Train Loss : 0.005 | Train Acc : 0.958 | Train Recall : 0.929 \n",
      "Epoch : 0 | Batch : 650 | Train Loss : 0.011 | Train Acc : 0.875 | Train Recall : 0.769 \n",
      "Epoch : 0 | Batch : 700 | Train Loss : 0.011 | Train Acc : 0.917 | Train Recall : 0.909 \n",
      "Epoch : 0 | Batch : 750 | Train Loss : 0.003 | Train Acc : 1.000 | Train Recall : 1.000 \n",
      "Epoch : 0 | Batch : 800 | Train Loss : 0.001 | Train Acc : 1.000 | Train Recall : 1.000 \n",
      "Epoch : 0 | Batch : 850 | Train Loss : 0.012 | Train Acc : 0.875 | Train Recall : 0.917 \n",
      "Epoch : 0 | Batch : 900 | Train Loss : 0.009 | Train Acc : 0.917 | Train Recall : 0.800 \n",
      "Epoch : 0 | Batch : 950 | Train Loss : 0.006 | Train Acc : 0.958 | Train Recall : 0.929 \n",
      "Epoch : 0 | Batch : 1000 | Train Loss : 0.006 | Train Acc : 0.958 | Train Recall : 0.917 \n",
      "Validation....\n",
      "Epoch : 0 | Train Loss : 0.013 | Train Acc : 0.867 | Val Loss : 0.010 | Val Acc : 0.908 | Recall : 0.869 | Precision : 0.942\n",
      "Epoch : 1 | Batch : 0 | Train Loss : 0.014 | Train Acc : 0.875 | Train Recall : 0.875 \n",
      "Epoch : 1 | Batch : 50 | Train Loss : 0.007 | Train Acc : 0.875 | Train Recall : 0.800 \n",
      "Epoch : 1 | Batch : 100 | Train Loss : 0.005 | Train Acc : 0.958 | Train Recall : 1.000 \n",
      "Epoch : 1 | Batch : 150 | Train Loss : 0.006 | Train Acc : 0.958 | Train Recall : 0.923 \n",
      "Epoch : 1 | Batch : 200 | Train Loss : 0.006 | Train Acc : 0.958 | Train Recall : 0.909 \n",
      "Epoch : 1 | Batch : 250 | Train Loss : 0.006 | Train Acc : 0.958 | Train Recall : 0.909 \n",
      "Epoch : 1 | Batch : 300 | Train Loss : 0.006 | Train Acc : 0.958 | Train Recall : 0.909 \n",
      "Epoch : 1 | Batch : 350 | Train Loss : 0.006 | Train Acc : 0.958 | Train Recall : 0.889 \n",
      "Epoch : 1 | Batch : 400 | Train Loss : 0.001 | Train Acc : 1.000 | Train Recall : 1.000 \n",
      "Epoch : 1 | Batch : 450 | Train Loss : 0.004 | Train Acc : 0.917 | Train Recall : 0.833 \n",
      "Epoch : 1 | Batch : 500 | Train Loss : 0.001 | Train Acc : 1.000 | Train Recall : 1.000 \n",
      "Epoch : 1 | Batch : 550 | Train Loss : 0.007 | Train Acc : 0.958 | Train Recall : 1.000 \n",
      "Epoch : 1 | Batch : 600 | Train Loss : 0.005 | Train Acc : 0.917 | Train Recall : 0.818 \n",
      "Epoch : 1 | Batch : 650 | Train Loss : 0.002 | Train Acc : 1.000 | Train Recall : 1.000 \n",
      "Epoch : 1 | Batch : 700 | Train Loss : 0.016 | Train Acc : 0.875 | Train Recall : 0.909 \n",
      "Epoch : 1 | Batch : 750 | Train Loss : 0.005 | Train Acc : 0.958 | Train Recall : 0.929 \n",
      "Epoch : 1 | Batch : 800 | Train Loss : 0.002 | Train Acc : 0.958 | Train Recall : 0.923 \n",
      "Epoch : 1 | Batch : 850 | Train Loss : 0.006 | Train Acc : 0.958 | Train Recall : 1.000 \n",
      "Epoch : 1 | Batch : 900 | Train Loss : 0.021 | Train Acc : 0.792 | Train Recall : 0.750 \n",
      "Epoch : 1 | Batch : 950 | Train Loss : 0.001 | Train Acc : 1.000 | Train Recall : 1.000 \n",
      "Epoch : 1 | Batch : 1000 | Train Loss : 0.006 | Train Acc : 0.958 | Train Recall : 1.000 \n",
      "Validation....\n",
      "Epoch : 1 | Train Loss : 0.005 | Train Acc : 0.957 | Val Loss : 0.004 | Val Acc : 0.970 | Recall : 0.953 | Precision : 0.986\n",
      "Epoch : 2 | Batch : 0 | Train Loss : 0.003 | Train Acc : 1.000 | Train Recall : 1.000 \n",
      "Epoch : 2 | Batch : 50 | Train Loss : 0.002 | Train Acc : 0.958 | Train Recall : 1.000 \n",
      "Epoch : 2 | Batch : 100 | Train Loss : 0.002 | Train Acc : 1.000 | Train Recall : 1.000 \n",
      "Epoch : 2 | Batch : 150 | Train Loss : 0.001 | Train Acc : 1.000 | Train Recall : 1.000 \n",
      "Epoch : 2 | Batch : 200 | Train Loss : 0.001 | Train Acc : 1.000 | Train Recall : 1.000 \n",
      "Epoch : 2 | Batch : 250 | Train Loss : 0.002 | Train Acc : 0.958 | Train Recall : 0.933 \n",
      "Epoch : 2 | Batch : 300 | Train Loss : 0.003 | Train Acc : 0.958 | Train Recall : 0.933 \n",
      "Epoch : 2 | Batch : 350 | Train Loss : 0.001 | Train Acc : 1.000 | Train Recall : 1.000 \n",
      "Epoch : 2 | Batch : 400 | Train Loss : 0.004 | Train Acc : 0.958 | Train Recall : 1.000 \n",
      "Epoch : 2 | Batch : 450 | Train Loss : 0.009 | Train Acc : 0.958 | Train Recall : 0.929 \n",
      "Epoch : 2 | Batch : 500 | Train Loss : 0.002 | Train Acc : 1.000 | Train Recall : 1.000 \n",
      "Epoch : 2 | Batch : 550 | Train Loss : 0.001 | Train Acc : 1.000 | Train Recall : 1.000 \n",
      "Epoch : 2 | Batch : 600 | Train Loss : 0.004 | Train Acc : 0.958 | Train Recall : 1.000 \n",
      "Epoch : 2 | Batch : 650 | Train Loss : 0.000 | Train Acc : 1.000 | Train Recall : 1.000 \n",
      "Epoch : 2 | Batch : 700 | Train Loss : 0.021 | Train Acc : 0.833 | Train Recall : 0.900 \n",
      "Epoch : 2 | Batch : 750 | Train Loss : 0.005 | Train Acc : 0.958 | Train Recall : 0.941 \n",
      "Epoch : 2 | Batch : 800 | Train Loss : 0.007 | Train Acc : 0.958 | Train Recall : 0.923 \n",
      "Epoch : 2 | Batch : 850 | Train Loss : 0.002 | Train Acc : 0.958 | Train Recall : 0.923 \n",
      "Epoch : 2 | Batch : 900 | Train Loss : 0.001 | Train Acc : 1.000 | Train Recall : 1.000 \n",
      "Epoch : 2 | Batch : 950 | Train Loss : 0.008 | Train Acc : 0.875 | Train Recall : 0.800 \n",
      "Epoch : 2 | Batch : 1000 | Train Loss : 0.003 | Train Acc : 0.958 | Train Recall : 0.923 \n",
      "Validation....\n",
      "Epoch : 2 | Train Loss : 0.004 | Train Acc : 0.974 | Val Loss : 0.003 | Val Acc : 0.979 | Recall : 0.960 | Precision : 0.997\n",
      "Epoch : 3 | Batch : 0 | Train Loss : 0.001 | Train Acc : 1.000 | Train Recall : 1.000 \n",
      "Epoch : 3 | Batch : 50 | Train Loss : 0.001 | Train Acc : 1.000 | Train Recall : 1.000 \n",
      "Epoch : 3 | Batch : 100 | Train Loss : 0.001 | Train Acc : 1.000 | Train Recall : 1.000 \n",
      "Epoch : 3 | Batch : 150 | Train Loss : 0.001 | Train Acc : 1.000 | Train Recall : 1.000 \n",
      "Epoch : 3 | Batch : 200 | Train Loss : 0.001 | Train Acc : 1.000 | Train Recall : 1.000 \n",
      "Epoch : 3 | Batch : 250 | Train Loss : 0.009 | Train Acc : 0.875 | Train Recall : 0.889 \n",
      "Epoch : 3 | Batch : 300 | Train Loss : 0.002 | Train Acc : 0.958 | Train Recall : 0.923 \n",
      "Epoch : 3 | Batch : 350 | Train Loss : 0.001 | Train Acc : 1.000 | Train Recall : 1.000 \n",
      "Epoch : 3 | Batch : 400 | Train Loss : 0.001 | Train Acc : 1.000 | Train Recall : 1.000 \n",
      "Epoch : 3 | Batch : 450 | Train Loss : 0.006 | Train Acc : 0.917 | Train Recall : 0.867 \n",
      "Epoch : 3 | Batch : 500 | Train Loss : 0.009 | Train Acc : 0.958 | Train Recall : 0.889 \n",
      "Epoch : 3 | Batch : 550 | Train Loss : 0.002 | Train Acc : 1.000 | Train Recall : 1.000 \n",
      "Epoch : 3 | Batch : 600 | Train Loss : 0.004 | Train Acc : 0.958 | Train Recall : 1.000 \n",
      "Epoch : 3 | Batch : 650 | Train Loss : 0.001 | Train Acc : 1.000 | Train Recall : 1.000 \n",
      "Epoch : 3 | Batch : 700 | Train Loss : 0.003 | Train Acc : 0.958 | Train Recall : 0.929 \n",
      "Epoch : 3 | Batch : 750 | Train Loss : 0.006 | Train Acc : 0.958 | Train Recall : 0.917 \n",
      "Epoch : 3 | Batch : 800 | Train Loss : 0.003 | Train Acc : 0.958 | Train Recall : 0.929 \n",
      "Epoch : 3 | Batch : 850 | Train Loss : 0.003 | Train Acc : 0.958 | Train Recall : 0.923 \n",
      "Epoch : 3 | Batch : 900 | Train Loss : 0.001 | Train Acc : 1.000 | Train Recall : 1.000 \n",
      "Epoch : 3 | Batch : 950 | Train Loss : 0.007 | Train Acc : 0.958 | Train Recall : 0.938 \n",
      "Epoch : 3 | Batch : 1000 | Train Loss : 0.001 | Train Acc : 1.000 | Train Recall : 1.000 \n",
      "Validation....\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    running_loss_train , accuracy_train = 0 , 0\n",
    "    \n",
    "    model_CNN8.train()\n",
    "    \n",
    "    for batch_idx, batch in enumerate(train_loader):\n",
    "        \n",
    "        optimizer_CNN8.zero_grad()\n",
    "        time_series , labels = batch\n",
    "        new_time_series, new_labels = time_series.to(device), labels.to(device)\n",
    "        preds = model_CNN8(new_time_series)\n",
    "        train_loss = criterion(preds, new_labels.float()) # float and unsqueeze for BCE\n",
    "        train_loss.backward()\n",
    "        optimizer_CNN8.step()\n",
    "        running_loss_train+=train_loss.item()/new_time_series.shape[0]\n",
    "        #accuracy = get_accuracy(preds, new_labels, batch_size)\n",
    "        accuracy = accuracy_score(labels, get_preds(preds))\n",
    "        accuracy_train+=accuracy\n",
    "        #recall = recall_score(new_labels.cpu(), preds.argmax(dim=1).cpu())\n",
    "        recall = recall_score(labels, get_preds(preds))\n",
    "        if batch_idx%50==0:\n",
    "            print('Epoch : %d | Batch : %d | Train Loss : %.3f | Train Acc : %.3f | Train Recall : %.3f ' % \n",
    "                  (epoch , batch_idx, train_loss.item() / new_time_series.shape[0], accuracy, recall))\n",
    "    \n",
    "    print(\"Validation....\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        model_CNN8.eval()\n",
    "        \n",
    "        #model_C.hidden = model_CRNN.init_hidden()\n",
    "        \n",
    "        running_loss_val, accuracy_val, recall_val , precision_val = 0 , 0 , 0 , 0\n",
    "        \n",
    "        for batch_idx_val, batch_val in enumerate(validation_loader):\n",
    "                                   \n",
    "            time_series_val , labels_val = batch_val\n",
    "            new_time_series_val, new_labels_val = time_series_val.to(device), labels_val.to(device)\n",
    "            preds_val = model_CNN8(new_time_series_val)\n",
    "            val_loss = criterion(preds_val, new_labels_val.float()) # float for BCE \n",
    "            running_loss_val+=val_loss.item() / new_time_series_val.shape[0]\n",
    "            #accuracy_= get_accuracy(preds_val , new_labels_val, batch_size)\n",
    "            accuracy_ = accuracy_score(labels_val , get_preds(preds_val))\n",
    "            accuracy_val+=accuracy_\n",
    "            #precision_ = precision_score(new_labels_val.cpu() , preds_val.argmax(dim=1).cpu())\n",
    "            precision_ = precision_score(labels_val , get_preds(preds_val))\n",
    "            precision_val+=precision_\n",
    "            #recall_ = recall_score(new_labels_val.cpu() , preds_val.argmax(dim=1).cpu())\n",
    "            recall_ = recall_score(labels_val , get_preds(preds_val))\n",
    "            recall_val += recall_\n",
    "    \n",
    "        \n",
    "        \n",
    "        print('Epoch : %d | Train Loss : %.3f | Train Acc : %.3f | Val Loss : %.3f | Val Acc : %.3f | Recall : %.3f | Precision : %.3f' \n",
    "              % (\n",
    "                  epoch , \n",
    "                  running_loss_train / len(train_loader),\n",
    "                  accuracy_train / len(train_loader),\n",
    "                  running_loss_val / len(validation_loader),\n",
    "                  accuracy_val / len(validation_loader),\n",
    "                  recall_val / len(validation_loader),\n",
    "                  precision_val / len(validation_loader)\n",
    "              )\n",
    "          )\n",
    "                    \n",
    "    epoch_list_CNN8.append(epoch)\n",
    "    val_loss_list_CNN8.append(running_loss_val / len(validation_loader))\n",
    "    accuracy_list_CNN8.append(accuracy_val / len(validation_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualization loss\n",
    "plt.plot(epoch_list, val_loss_list)\n",
    "plt.xlabel(\"# of epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"CNN4: Loss vs # epochs\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# visualization accuracy\n",
    "plt.plot(epoch_list, accuracy_list, color=\"red\")\n",
    "plt.xlabel(\"# of epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"CNN4: Accuracy vs # epochs\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
